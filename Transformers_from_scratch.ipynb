{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T03:32:11.078339Z",
     "iopub.status.busy": "2022-08-14T03:32:11.077710Z",
     "iopub.status.idle": "2022-08-14T03:32:12.485825Z",
     "shell.execute_reply": "2022-08-14T03:32:12.484850Z",
     "shell.execute_reply.started": "2022-08-14T03:32:11.078227Z"
    }
   },
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math,copy\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we need to convert each word in the input sequence to an embedding vector. Embedding vectors will create a more semantic representation of each word.\n",
    "\n",
    "Suppoese each embedding vector is of 512 dimension and suppose our vocab size is 100, then our embedding matrix will be of size 100x512. These marix will be learned on training and during inference each word will be mapped to corresponding 512 d vector. Suppose we have batch size of 32 and sequence length of 10(10 words). The the output will be 32x10x512.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T03:32:12.487666Z",
     "iopub.status.busy": "2022-08-14T03:32:12.487437Z",
     "iopub.status.idle": "2022-08-14T03:32:12.494619Z",
     "shell.execute_reply": "2022-08-14T03:32:12.493560Z",
     "shell.execute_reply.started": "2022-08-14T03:32:12.487638Z"
    }
   },
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: size of vocabulary\n",
    "            embed_dim: dimension of embeddings\n",
    "        \"\"\"\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input vector\n",
    "        Returns:\n",
    "            out: embedding vector\n",
    "        \"\"\"\n",
    "        out = self.embed(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/524/1*yWGV9ck-0ltfV2wscUeo7Q.png\">\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/564/1*SgNlyFaHH8ljBbpCupDhSQ.png\">\n",
    "\n",
    "```\n",
    "pos -> refers to order in the sentence\n",
    "i -> refers to position along embedding vector dimension\n",
    "```\n",
    "\n",
    "Positinal embedding will generate a matrix of similar to embedding matrix. It will create a matrix of dimension sequence length x embedding dimension. For each token(word) in sequence, we will find the embedding vector which is of dimension 1 x 512 and it is added with the correspondng positional vector which is of dimension 1 x 512 to get 1 x 512 dim out for each word/token.\n",
    "\n",
    "for eg: if we have batch size of 32 and seq length of 10 and let embedding dimension be 512. Then we will have embedding vector of dimension 32 x 10 x 512. Similarly we will have positional encoding vector of dimension 32 x 10 x 512. Then we add both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T03:32:12.495880Z",
     "iopub.status.busy": "2022-08-14T03:32:12.495665Z",
     "iopub.status.idle": "2022-08-14T03:32:12.506914Z",
     "shell.execute_reply": "2022-08-14T03:32:12.506146Z",
     "shell.execute_reply.started": "2022-08-14T03:32:12.495854Z"
    }
   },
   "outputs": [],
   "source": [
    "# register buffer in Pytorch ->\n",
    "# If you have parameters in your model, which should be saved and restored in the state_dict,\n",
    "# but not trained by the optimizer, you should register them as buffers.\n",
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self,max_seq_len,embed_model_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            seq_len: length of input sequence\n",
    "            embed_model_dim: demension of embedding\n",
    "        \"\"\"\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.embed_dim = embed_model_dim\n",
    "\n",
    "        pe = torch.zeros(max_seq_len,self.embed_dim)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0,self.embed_dim,2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/self.embed_dim)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/self.embed_dim)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input vector\n",
    "        Returns:\n",
    "            x: output\n",
    "        \"\"\"\n",
    "      \n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.embed_dim)\n",
    "        #add constant to embedding\n",
    "        seq_len = x.size(1)\n",
    "        x = x + torch.autograd.Variable(self.pe[:,:seq_len], requires_grad=False)\n",
    "        return x\n",
    "               \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As model proceeses each word, self attention allows it to look at other positions in the input sequence for clues. It will creates a vector based on dependency of each word with the other.\n",
    "\n",
    "\n",
    "Let us go through a step by step illustration of self attention.\n",
    "\n",
    "* **Step 1:** The first step in calculating self-attention is to create three vectors from each of the encoderâ€™s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. Each of the vector will be of dimension 1x64.\n",
    "\n",
    "Since we have a multihead attention we will have 8 self attention heads.I will explain the code with 8 attention head in mind.\n",
    "\n",
    "**How key,queries and values can be created?**\n",
    "\n",
    "We will have a key matrix,query matrix and a value matrix to generate key, query and value.\n",
    "These matrixes are learned during training.\n",
    "\n",
    "```\n",
    "code hint:\n",
    "Suppose we have batch_size=32,sequence_length=10, embedding dimension=512. So after embedding and positional encoding our output will be of dimension 32x10x512.\n",
    "We will resize it to 32x10x8x16.(About 8, it is the number of heads in multihead attention.Dont worry you will get to know about it once you go through the code.).\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "code hint:\n",
    "Suppose our key,query and value dimension be 32x10x8x64. Before proceeding further, we will transpose each of them for multiplication convinience (32x8x10x64). Now multiply query matrix with transpose key matrix. ie (32x8x10x64) x (32x8x64x10) -> (32x8x10x10).\n",
    "```\n",
    "\n",
    "\n",
    "* **Step 3:** Now divide the output matrix with dimension of key matrix and then apply Softmax over it.\n",
    "\n",
    "\n",
    "* **Step 4:** Then this gets multiply it with value matrix.\n",
    "\n",
    "```\n",
    "code hint:\n",
    "After step 3 our output will be of dimension 32x8x10x10. Now muliply it with value matrix (32x8x10x64) to get output of dimension (32x8x10x64).Here 8 is the number of attention heads and 10 is the sequence length.Thus for each word we have 64 dim vector.\n",
    "```\n",
    "\n",
    "* **Step 5:** Once we have this we will pass this through a linear layer. This forms the output of multihead attention.\n",
    "\n",
    "```\n",
    "code hint:\n",
    "(32x8x10x64) vector gets transposed to (32x10x8x64) and then reshaped as (32x10x512).Then it is passed through a linear layer to get output of (32x10x512).\n",
    "```\n",
    "\n",
    "\n",
    "Now you got an idea on how multihead attention works. You will be more clear once you go through the implementation part of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T03:32:12.510803Z",
     "iopub.status.busy": "2022-08-14T03:32:12.509994Z",
     "iopub.status.idle": "2022-08-14T03:32:12.527635Z",
     "shell.execute_reply": "2022-08-14T03:32:12.526693Z",
     "shell.execute_reply.started": "2022-08-14T03:32:12.510756Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim=512, n_heads=8):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embed_dim: dimension of embeding vector output\n",
    "            n_heads: number of self attention heads\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim    #512 dim\n",
    "        self.n_heads = n_heads   #8\n",
    "        self.single_head_dim = int(self.embed_dim / self.n_heads)   #512/8 = 64  . each key,query, value will be of 64d\n",
    "       \n",
    "        #key,query and value matrixes    #64 x 64   \n",
    "        self.query_matrix = nn.Linear(self.single_head_dim , self.single_head_dim ,bias=False)  # single key matrix for all 8 keys #512x512\n",
    "        self.key_matrix = nn.Linear(self.single_head_dim  , self.single_head_dim, bias=False)\n",
    "        self.value_matrix = nn.Linear(self.single_head_dim ,self.single_head_dim , bias=False)\n",
    "        self.out = nn.Linear(self.n_heads*self.single_head_dim ,self.embed_dim) \n",
    "\n",
    "    def forward(self,key,query,value,mask=None):    #batch_size x sequence_length x embedding_dim    # 32 x 10 x 512\n",
    "        \n",
    "        \"\"\"\n",
    "           key : key vector\n",
    "           query : query vector\n",
    "           value : value vector\n",
    "           mask: mask for decoder\n",
    "        \n",
    "        Returns:\n",
    "           output vector from multihead attention\n",
    "        \"\"\"\n",
    "        batch_size = key.size(0)\n",
    "        seq_length = key.size(1)\n",
    "        # 32x10x512\n",
    "        key = key.view(batch_size, seq_length, self.n_heads, self.single_head_dim)  #batch_size x sequence_length x n_heads x single_head_dim = (32x10x8x64)\n",
    "        query = query.view(batch_size, seq_length, self.n_heads, self.single_head_dim) #(32x10x8x64)\n",
    "        value = value.view(batch_size, seq_length, self.n_heads, self.single_head_dim) #(32x10x8x64)\n",
    "       \n",
    "        k = self.key_matrix(key)       # (32x10x8x64)\n",
    "        q = self.query_matrix(query)   \n",
    "        v = self.value_matrix(value)\n",
    "\n",
    "        q = q.transpose(1,2)  # (batch_size, n_heads, seq_len, single_head_dim)    # (32 x 8 x 10 x 64)\n",
    "        k = k.transpose(1,2)  # (batch_size, n_heads, seq_len, single_head_dim)\n",
    "        v = v.transpose(1,2)  # (batch_size, n_heads, seq_len, single_head_dim)\n",
    "       \n",
    "        # computes attention\n",
    "        # adjust key for matrix multiplication\n",
    "        k_adjusted = k.transpose(-1,-2)  #(batch_size, n_heads, single_head_dim, seq_ken)  #(32 x 8 x 64 x 10)\n",
    "        product = torch.matmul(q, k_adjusted)  #(32 x 8 x 10 x 64) x (32 x 8 x 64 x 10) = #(32x8x10x10)\n",
    "      \n",
    "       \n",
    "        if mask is not None:\n",
    "             product = product.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        #divising by square root of key dimension\n",
    "        product = product / math.sqrt(self.single_head_dim) # / sqrt(64)\n",
    "\n",
    "        #applying softmax\n",
    "        scores = F.softmax(product, dim=-1)\n",
    "\n",
    "        #mutiply with value matrix\n",
    "        scores = torch.matmul(scores, v)  ##(32x8x 10x 10) x (32 x 8 x 10 x 64) = (32 x 8 x 10 x 64) \n",
    "       \n",
    "        #concatenated output\n",
    "        concat = scores.transpose(1,2).contiguous().view(batch_size, seq_length, self.single_head_dim*self.n_heads)  # (32x8x10x64) -> (32x10x8x64)  -> (32,10,512)\n",
    "        \n",
    "        output = self.out(concat) #(32,10,512) -> (32,10,512)\n",
    "       \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T03:32:12.530078Z",
     "iopub.status.busy": "2022-08-14T03:32:12.529112Z",
     "iopub.status.idle": "2022-08-14T03:32:12.545028Z",
     "shell.execute_reply": "2022-08-14T03:32:12.543961Z",
     "shell.execute_reply.started": "2022-08-14T03:32:12.530033Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion_factor=4, n_heads=8):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "           embed_dim: dimension of the embedding\n",
    "           expansion_factor: fator ehich determines output dimension of linear layer\n",
    "           n_heads: number of attention heads\n",
    "        \n",
    "        \"\"\"\n",
    "        self.attention = MultiHeadAttention(embed_dim, n_heads)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(embed_dim) \n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "                          nn.Linear(embed_dim, expansion_factor*embed_dim),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(expansion_factor*embed_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self,key,query,value,mask=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "           key: key vector\n",
    "           query: query vector\n",
    "           value: value vector\n",
    "           mask: mask to be given for multi head attnetion(used only for the decoder)\n",
    "        Returns:\n",
    "           norm2_out: output of transformer block\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        attention_out = self.attention(key,query,value,mask)  #32x10x512\n",
    "        attention_residual_out = attention_out + value  #32x10x512\n",
    "        norm1_out = self.dropout1(self.norm1(attention_residual_out)) #32x10x512\n",
    "\n",
    "        feed_fwd_out = self.feed_forward(norm1_out) #32x10x512 -> #32x10x2048 -> 32x10x512\n",
    "        feed_fwd_residual_out = feed_fwd_out + norm1_out #32x10x512\n",
    "        norm2_out = self.dropout2(self.norm2(feed_fwd_residual_out)) #32x10x512\n",
    "\n",
    "        return norm2_out\n",
    "\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        seq_len : length of input sequence\n",
    "        embed_dim: dimension of embedding\n",
    "        num_layers: number of encoder layers\n",
    "        expansion_factor: factor which determines number of linear layers in feed forward layer\n",
    "        n_heads: number of heads in multihead attention\n",
    "        \n",
    "    Returns:\n",
    "        out: output of the encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_len, vocab_size, embed_dim, num_layers=2, expansion_factor=4, n_heads=8):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.embedding_layer = Embedding(vocab_size, embed_dim)\n",
    "        self.positional_encoder = PositionalEmbedding(seq_len, embed_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([TransformerBlock(embed_dim, expansion_factor, n_heads) for i in range(num_layers)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embed_out = self.embedding_layer(x)\n",
    "        out = self.positional_encoder(embed_out)\n",
    "        for layer in self.layers:\n",
    "            out = layer(out,out,out)\n",
    "\n",
    "        return out  #32x10x512\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next we have another multihead attention and then a add and norm layer. This multihead attention is called encoder-decorder multihead attention. For this multihead attention we create we create key and query vectors from the encoder output. Value is created from the output of previous decoder layer.\n",
    "\n",
    "```\n",
    "remember:\n",
    "Thus we have 32x10x512 out from encoder out. key and query for all words are generated from it. Similary value matrix is generated from otput from previous layer of decoder(32x10x512).\n",
    "\n",
    "```\n",
    "\n",
    "Thus it is passed through a multihead atention (we used number of heads = 8) the through a Add and Norm layer. Here the output from previous encoder layer(ie previoud add and norm layer) gets added with encoder-decoder attention output and then normalized.\n",
    "\n",
    "Next we have a feed forward layer(linear layer) with add and nom which is similar to that of present in the encoder.\n",
    "\n",
    "\n",
    "\n",
    "Finally we create a linear layer with length equal to number of words in total target corpus and a softmax function with it to get probablity of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T03:32:12.547482Z",
     "iopub.status.busy": "2022-08-14T03:32:12.546540Z",
     "iopub.status.idle": "2022-08-14T03:32:12.563125Z",
     "shell.execute_reply": "2022-08-14T03:32:12.562518Z",
     "shell.execute_reply.started": "2022-08-14T03:32:12.547435Z"
    }
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, expansion_factor=4, n_heads=8):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "           embed_dim: dimension of the embedding\n",
    "           expansion_factor: fator ehich determines output dimension of linear layer\n",
    "           n_heads: number of attention heads\n",
    "        \n",
    "        \"\"\"\n",
    "        self.attention = MultiHeadAttention(embed_dim, n_heads=8)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.transformer_block = TransformerBlock(embed_dim, expansion_factor, n_heads)\n",
    "        \n",
    "    \n",
    "    def forward(self, key, query, x, mask):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "           key: key vector\n",
    "           query: query vector\n",
    "           value: value vector\n",
    "           mask: mask to be given for multi head attention \n",
    "        Returns:\n",
    "           out: output of transformer block\n",
    "    \n",
    "        \"\"\"\n",
    "        attention = self.attention(x,x,x,mask) #32x10x512\n",
    "        value = self.dropout(self.norm(attention + x))\n",
    "        out = self.transformer_block(key, query, value, mask)\n",
    "\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, target_vocab_size, embed_dim, seq_len, num_layers=2, expansion_factor=4, n_heads=8):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        \"\"\"  \n",
    "        Args:\n",
    "           target_vocab_size: vocabulary size of taget\n",
    "           embed_dim: dimension of embedding\n",
    "           seq_len : length of input sequence\n",
    "           num_layers: number of encoder layers\n",
    "           expansion_factor: factor which determines number of linear layers in feed forward layer\n",
    "           n_heads: number of heads in multihead attention\n",
    "        \n",
    "        \"\"\"\n",
    "        self.word_embedding = nn.Embedding(target_vocab_size, embed_dim)\n",
    "        self.position_embedding = PositionalEmbedding(seq_len, embed_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(embed_dim, expansion_factor=4, n_heads=8) \n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_dim, target_vocab_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x, enc_out, trg_mask):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input vector from target\n",
    "            enc_out : output from encoder layer\n",
    "            trg_mask: mask for decoder self attention\n",
    "        Returns:\n",
    "            out: output vector\n",
    "        \"\"\"\n",
    "        batch_size, seq_length = x.shape[0],x.shape[1]  #32x10\n",
    "\n",
    "        x = self.word_embedding(x)  #32x10x512\n",
    "        x = self.position_embedding(x) #32x10x512\n",
    "        x = self.dropout(x)\n",
    "     \n",
    "        for layer in self.layers:\n",
    "            x = layer(enc_out, enc_out, x, trg_mask) \n",
    "\n",
    "        out = F.softmax(self.fc_out(x))\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will arrange all submodules and creates the entire tranformer architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T03:32:12.565217Z",
     "iopub.status.busy": "2022-08-14T03:32:12.564456Z",
     "iopub.status.idle": "2022-08-14T03:32:12.578091Z",
     "shell.execute_reply": "2022-08-14T03:32:12.577219Z",
     "shell.execute_reply.started": "2022-08-14T03:32:12.565174Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embed_dim, src_vocab_size, target_vocab_size, seq_length,num_layers=2, expansion_factor=4, n_heads=8):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        \"\"\"  \n",
    "        Args:\n",
    "           embed_dim:  dimension of embedding \n",
    "           src_vocab_size: vocabulary size of source\n",
    "           target_vocab_size: vocabulary size of target\n",
    "           seq_length : length of input sequence\n",
    "           num_layers: number of encoder layers\n",
    "           expansion_factor: factor which determines number of linear layers in feed forward layer\n",
    "           n_heads: number of heads in multihead attention\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "\n",
    "        self.encoder = TransformerEncoder(seq_length, src_vocab_size, embed_dim, num_layers=num_layers, expansion_factor=expansion_factor, n_heads=n_heads)\n",
    "        self.decoder = TransformerDecoder(target_vocab_size, embed_dim, seq_length, num_layers=num_layers, expansion_factor=expansion_factor, n_heads=n_heads)\n",
    "\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            trg: target sequence\n",
    "        Returns:\n",
    "            trg_mask: target mask\n",
    "        \"\"\"\n",
    "        batch_size, trg_len = trg.shape\n",
    "        # returns the lower triangular part of matrix filled with ones\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            batch_size, 1, trg_len, trg_len\n",
    "        )\n",
    "        return trg_mask    \n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: input to encoder \n",
    "            trg: input to decoder\n",
    "        out:\n",
    "            out: final vector which returns probabilities of each target word\n",
    "        \"\"\"\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_src = self.encoder(src)\n",
    "        out = self.decoder(trg, enc_src, trg_mask)\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Suppose we have input sequence oflength 9 and target sequence of length 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T03:32:12.581787Z",
     "iopub.status.busy": "2022-08-14T03:32:12.581393Z",
     "iopub.status.idle": "2022-08-14T03:32:12.598117Z",
     "shell.execute_reply": "2022-08-14T03:32:12.597368Z",
     "shell.execute_reply.started": "2022-08-14T03:32:12.581751Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]])\n",
    "target = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0, 1,9], [1, 5, 6, 2, 4, 7, 6, 2, 0,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T03:32:12.600305Z",
     "iopub.status.busy": "2022-08-14T03:32:12.599417Z",
     "iopub.status.idle": "2022-08-14T03:32:13.009590Z",
     "shell.execute_reply": "2022-08-14T03:32:13.008530Z",
     "shell.execute_reply.started": "2022-08-14T03:32:12.600260Z"
    }
   },
   "outputs": [],
   "source": [
    "src_vocab_size = 10\n",
    "target_vocab_size = 10\n",
    "num_layers = 6\n",
    "seq_length= 9\n",
    "\n",
    "model = Transformer(embed_dim=512, src_vocab_size=src_vocab_size, target_vocab_size=target_vocab_size, seq_length=seq_length, num_layers=num_layers, expansion_factor=4, n_heads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T03:32:13.011813Z",
     "iopub.status.busy": "2022-08-14T03:32:13.011589Z",
     "iopub.status.idle": "2022-08-14T03:32:13.023630Z",
     "shell.execute_reply": "2022-08-14T03:32:13.022576Z",
     "shell.execute_reply.started": "2022-08-14T03:32:13.011786Z"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T03:32:13.025612Z",
     "iopub.status.busy": "2022-08-14T03:32:13.025311Z",
     "iopub.status.idle": "2022-08-14T03:32:13.160708Z",
     "shell.execute_reply": "2022-08-14T03:32:13.159901Z",
     "shell.execute_reply.started": "2022-08-14T03:32:13.025572Z"
    }
   },
   "outputs": [],
   "source": [
    "out = model(x, target[:, :-1])\n",
    "print(x.shape,target.shape)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a class=\"anchor\" id=\"section10\"></a>\n",
    "<h2 style=\"color:green;font-size: 2em;\"> 10.  Some useful resources </h2>\n",
    "\n",
    "* Understanding transformers\n",
    "  - https://theaisummer.com/transformer/\n",
    "  - https://jalammar.github.io/illustrated-transformer/\n",
    "* Pytorch implementation\n",
    "  - https://www.youtube.com/watch?v=U0s0f995w14"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
